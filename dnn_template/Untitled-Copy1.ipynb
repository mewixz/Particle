{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d7402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ba0f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Data Preparing\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Data Preparing\")\n",
    "pixels = [\"img_{0}\".format(i) for i in range(1600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4787af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_image(df):\n",
    "    return np.expand_dims(df[pixels], axis=-1).reshape(-1, 40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427d7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_image_2d(df):\n",
    "    foo =  np.expand_dims(np.expand_dims(df[ [\"img_{0}\".format(i) for i in range(40*40)]], axis=-1).reshape(-1,40,40), axis=-1)        \n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d9c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95c085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_train = pd.HDFStore('../train_image.h5')\n",
    "df_train_images = store_train.select(\"table\",stop=1200000)\n",
    "n_train_samples = int((store_train.get_storer('table').nrows/batch_size))*batch_size\n",
    "\n",
    "\n",
    "store_val = pd.HDFStore('../val_image.h5')\n",
    "df_val_images = store_val.select(\"table\",stop=1200000)\n",
    "n_val_samples = int((store_val.get_storer('table').nrows/batch_size))*batch_size\n",
    "\n",
    "store_test = pd.HDFStore('../test_image.h5')\n",
    "df_test_images = store_test.select(\"table\",stop=1200000)\n",
    "n_test_samples = int((store_test.get_storer('table').nrows/batch_size))*batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42791a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_0</th>\n",
       "      <th>img_1</th>\n",
       "      <th>img_2</th>\n",
       "      <th>img_3</th>\n",
       "      <th>img_4</th>\n",
       "      <th>img_5</th>\n",
       "      <th>img_6</th>\n",
       "      <th>img_7</th>\n",
       "      <th>img_8</th>\n",
       "      <th>img_9</th>\n",
       "      <th>...</th>\n",
       "      <th>img_1592</th>\n",
       "      <th>img_1593</th>\n",
       "      <th>img_1594</th>\n",
       "      <th>img_1595</th>\n",
       "      <th>img_1596</th>\n",
       "      <th>img_1597</th>\n",
       "      <th>img_1598</th>\n",
       "      <th>img_1599</th>\n",
       "      <th>is_signal_new</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.394016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.997341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.648197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.098465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.350220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_0  img_1  img_2  img_3  img_4  img_5  img_6  img_7  img_8  img_9  ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   img_1592  img_1593  img_1594  img_1595  img_1596  img_1597  img_1598  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   img_1599  is_signal_new        mass  \n",
       "0       0.0            0.0   37.394016  \n",
       "1       0.0            0.0   46.997341  \n",
       "2       0.0            0.0   61.648197  \n",
       "3       0.0            0.0   52.098465  \n",
       "4       0.0            0.0  174.350220  \n",
       "\n",
       "[5 rows x 1602 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae31e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_0</th>\n",
       "      <th>img_1</th>\n",
       "      <th>img_2</th>\n",
       "      <th>img_3</th>\n",
       "      <th>img_4</th>\n",
       "      <th>img_5</th>\n",
       "      <th>img_6</th>\n",
       "      <th>img_7</th>\n",
       "      <th>img_8</th>\n",
       "      <th>img_9</th>\n",
       "      <th>...</th>\n",
       "      <th>img_1592</th>\n",
       "      <th>img_1593</th>\n",
       "      <th>img_1594</th>\n",
       "      <th>img_1595</th>\n",
       "      <th>img_1596</th>\n",
       "      <th>img_1597</th>\n",
       "      <th>img_1598</th>\n",
       "      <th>img_1599</th>\n",
       "      <th>is_signal_new</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43221</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.839817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.690308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.865814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.494675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.776985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       img_0  img_1  img_2  img_3  img_4  img_5  img_6  img_7  img_8  img_9  \\\n",
       "43221    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "77280    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "78615    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "26383    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "789      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...  img_1592  img_1593  img_1594  img_1595  img_1596  img_1597  \\\n",
       "43221  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "77280  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "78615  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "26383  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "789    ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       img_1598  img_1599  is_signal_new        mass  \n",
       "43221       0.0       0.0            0.0   42.839817  \n",
       "77280       0.0       0.0            0.0   71.690308  \n",
       "78615       0.0       0.0            0.0  192.865814  \n",
       "26383       0.0       0.0            0.0  111.494675  \n",
       "789         0.0       0.0            0.0  111.776985  \n",
       "\n",
       "[5 rows x 1602 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1596a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_batch_h5(brs, store, batch_size=1024):\n",
    "    \"\"\"Generates data in batchaes using partial reading of h5 files \"\"\"\n",
    "\n",
    "    size = store.get_storer('table').nrows    \n",
    "    i_start = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        if size >= i_start+batch_size:            \n",
    "            foo = store.select('table',\n",
    "                               columns = brs,\n",
    "                               start = i_start,\n",
    "                               stop  = i_start + batch_size)\n",
    "            yield foo\n",
    "            i_start += batch_size\n",
    "            step += 1\n",
    "        else:\n",
    "            size = store.get_storer('table').nrows\n",
    "            i_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ac58b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train = datagen_batch_h5('is_signal_new', store_train, batch_size = batch_size)\n",
    "datagen_test  = datagen_batch_h5('is_signal_new', store_test, batch_size = batch_size)\n",
    "datagen_val   = datagen_batch_h5('is_signal_new', store_val, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6aefb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = next(datagen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56d80735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c89f8a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['img_0', 'img_1', 'img_2', 'img_3', 'img_4', 'img_5', 'img_6', 'img_7',\\n       'img_8', 'img_9',\\n       ...\\n       'img_1590', 'img_1591', 'img_1592', 'img_1593', 'img_1594', 'img_1595',\\n       'img_1596', 'img_1597', 'img_1598', 'img_1599'],\\n      dtype='object', length=1600)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m      3\u001b[0m         (\n\u001b[0;32m----> 4\u001b[0m             tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mdf_train_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m), tf\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m      5\u001b[0m             tf\u001b[38;5;241m.\u001b[39mcast(df_train_images[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_signal_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m     12\u001b[0m         (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.8/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.8/site-packages/pandas/core/indexes/base.py:5842\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5841\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['img_0', 'img_1', 'img_2', 'img_3', 'img_4', 'img_5', 'img_6', 'img_7',\\n       'img_8', 'img_9',\\n       ...\\n       'img_1590', 'img_1591', 'img_1592', 'img_1593', 'img_1594', 'img_1595',\\n       'img_1596', 'img_1597', 'img_1598', 'img_1599'],\\n      dtype='object', length=1600)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_train_images[pixels].values.reshape(-1, 40, 40), tf.float32),\n",
    "            tf.cast(df_train_images['is_signal_new'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_val_images[pixels].values.reshape(-1, 40, 40), tf.float32),\n",
    "            tf.cast(df_val_images['is_signal_new'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc119e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_constit = 2\n",
    "feat_list =  [\"E\",\"PX\",\"PY\",\"PZ\"]\n",
    "cols = [\"{0}_{1}\".format(feature,constit)\n",
    "        for feature in feat_list for constit in range(n_constit)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9a7baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E_0', 'E_1', 'PX_0', 'PX_1', 'PY_0', 'PY_1', 'PZ_0', 'PZ_1']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a1e83dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec4 = np.expand_dims(df_train_images[cols],axis=-1).reshape(-1, len(feat_list), n_constit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc6c096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1, 4, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1, len(feat_list), n_constit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10165730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 474.07114 ,  103.23624 ],\n",
       "        [-250.34703 ,  -48.866222],\n",
       "        [-223.65196 ,  -56.790775],\n",
       "        [-334.7381  ,  -71.02549 ]],\n",
       "\n",
       "       [[ 150.50453 ,   82.25706 ],\n",
       "        [ 120.06239 ,   63.80174 ],\n",
       "        [  76.852005,   42.754807],\n",
       "        [ -48.274265,  -29.454842]],\n",
       "\n",
       "       [[ 251.64539 ,  104.1478  ],\n",
       "        [  10.427651,   10.718256],\n",
       "        [-147.57375 ,  -54.497948],\n",
       "        [ 203.56488 ,   88.101395]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  64.15237 ,   50.308674],\n",
       "        [  47.883705,   45.10902 ],\n",
       "        [  17.668383,   20.591967],\n",
       "        [  38.865215,    8.49174 ]],\n",
       "\n",
       "       [[ 132.56255 ,   52.84212 ],\n",
       "        [ 103.131134,   41.600754],\n",
       "        [  32.62031 ,   13.222459],\n",
       "        [ -76.63363 ,  -29.77975 ]],\n",
       "\n",
       "       [[  83.333534,   80.80607 ],\n",
       "        [  -9.745   ,   -8.434953],\n",
       "        [ -82.62369 ,  -74.78946 ],\n",
       "        [  -4.778968,  -29.411036]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a6207f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "E     = vec4[:,0,:]\n",
    "pxs   = vec4[:,1,:]\n",
    "pys   = vec4[:,2,:]\n",
    "pzs   = vec4[:,3,:]\n",
    "pT    = np.sqrt(pxs**2+pys**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a3fc877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200000, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5a0052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_brs = []\n",
    "pixel_brs += [\"img_{0}\".format(i) for i in range(40*40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af63bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pixel_brs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d28fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[np.random.permutation(len(df))]\n",
    "X = datagen_train\n",
    "y = tf.keras.utils.to_categorical(df[clf.params[\"signal_branch\"]].values,clf.params[\"n_classes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15865b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ad3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen_batch_h5(brs, store, batch_size=1024):\n",
    "    \"\"\"Generates data in batchaes using partial reading of h5 files \"\"\"\n",
    "\n",
    "    size = store.get_storer('table').nrows    \n",
    "    i_start = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        if size >= i_start+batch_size:            \n",
    "            foo = store.select('table',\n",
    "                               columns = brs,\n",
    "                               start = i_start,\n",
    "                               stop  = i_start + batch_size)\n",
    "            yield foo\n",
    "            i_start += batch_size\n",
    "            step += 1\n",
    "        else:\n",
    "            size = store.get_storer('table').nrows\n",
    "            i_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfaf85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples =  1210976\n",
      "Total number of testing samples =  402976\n",
      "Total number of valing samples =  404000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of training samples = \", n_train_samples)\n",
    "print(\"Total number of testing samples = \", n_test_samples)\n",
    "print(\"Total number of valing samples = \", n_val_samples)\n",
    "samples_per_epoch = n_train_samples\n",
    "samples_per_epoch_test = n_test_samples\n",
    "samples_per_epoch_val = n_val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23691ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f5ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2d(params):\n",
    "    activ = lambda : Activation('relu')\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\",input_shape=(40, 40, 1)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(400,activation=\"relu\"))\n",
    "    model.add(layers.Dense(200,activation=\"relu\"))\n",
    "    model.add(layers.Dense(100,activation=\"relu\"))\n",
    "    model.add(layers.Dense(50,activation=\"relu\"))\n",
    "    model.add(layers.Dense(10,activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed09918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {}\n",
    "for i in range(2):\n",
    "    class_names[i] = \"c{0}\".format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce12082d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'c0', 1: 'c1'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71234b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "train_gen = generator(clf.datagen_train)\n",
    "val_gen  = generator(clf.datagen_val) # CORRECTED\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                               patience=clf.params[\"early_stop_patience\"], \n",
    "                               verbose=0, \n",
    "                               mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f8da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras(clf):\n",
    "\n",
    "    print(\"Starting train_keras with the parameters: \")\n",
    "    for k,v in clf.params.items():\n",
    "        print(\"\\t\", k,\"=\",v)\n",
    "\n",
    "    outdir = clf.params[\"output_path\"] + clf.name\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "      \n",
    "    # Prepare model and train\n",
    "    #sgd = SGD(lr = clf.params[\"lr\"], \n",
    "    #          decay = clf.params[\"decay\"], \n",
    "    #          momentum = clf.params[\"momentum\"], \n",
    "    #          nesterov=True)\n",
    "    clf.model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=\"adam\", \n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    if clf.params[\"reweight_events\"]:\n",
    "        f = open(\"mass_weights.pickle\", \"rb\")\n",
    "        di = pickle.load(f)\n",
    "        \n",
    "        counts_qcd  = di[\"counts_qcd\"]\n",
    "        bins_qcd    = di[\"bins_qcd\"]\n",
    "        counts_top  = di[\"counts_top\"]\n",
    "        bins_top    = di[\"bins_top\"]\n",
    "       \n",
    "        \n",
    "        bins_qcd = bins_qcd[:-1]+(bins_qcd[1:]-bins_qcd[:-1])/2.\n",
    "        bins_top = bins_top[:-1]+(bins_top[1:]-bins_top[:-1])/2.\n",
    "     \n",
    "        bins = [bins_qcd, bins_top]\n",
    "        counts = [counts_qcd, counts_top]\n",
    "        \n",
    "        c_min =[min(counts[0][np.nonzero(counts[0])]), min(counts[1][np.nonzero(counts[1])])]\n",
    "\n",
    "        def calc_w(m,s):\n",
    "\n",
    "            if m>max(bins[s]) or m<min(bins[s]):\n",
    "                return 1/c_min[s]\n",
    "\n",
    "            c = counts[s][np.abs(m-bins[s]).argmin()]\n",
    "            c = max(c_min[s],c)\n",
    "\n",
    "            return 1/c\n",
    "\n",
    "\n",
    "    print(\"Calling fit_generator\")\n",
    "\n",
    "    def generator(dg):\n",
    "        while True:\n",
    "            df = next(dg)\n",
    "            \n",
    "            # Shuffle\n",
    "            df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "            X = clf.image_fun(df)\n",
    "            y = np_utils.to_categorical(df[clf.params[\"signal_branch\"]].values,clf.params[\"n_classes\"])\n",
    "\n",
    "            if clf.params[\"reweight_events\"]:\n",
    "                \n",
    "                Es = [\"E_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PXs = [\"PX_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PYs = [\"PY_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PZs = [\"PZ_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "\n",
    "                E_tot = df[Es].sum(axis=1)\n",
    "                PX_tot = df[PXs].sum(axis=1)\n",
    "                PY_tot = df[PYs].sum(axis=1)\n",
    "                PZ_tot = df[PZs].sum(axis=1)\n",
    "\n",
    "                M = np.sqrt(np.power(E_tot,2) - np.power(PX_tot,2) - np.power(PY_tot,2) - np.power(PZ_tot,2))\n",
    "                w = np.array([calc_w(m,s) for m,s in zip(M,df[\"is_signal_new\"])])\n",
    "                        \n",
    "                yield  X,y, w\n",
    "\n",
    "            else:        \n",
    "                yield X,y\n",
    "\n",
    "                \n",
    "    train_gen = generator(clf.datagen_train)\n",
    "    val_gen  = generator(clf.datagen_val) # CORRECTED\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', \n",
    "                               patience=clf.params[\"early_stop_patience\"], \n",
    "                               verbose=0, \n",
    "                               mode='auto')\n",
    "\n",
    "    filepath= outdir + \"/weights-latest.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    # save the architecture\n",
    "    model_out_yaml = open(outdir + \"/\" + clf.name + \".yaml\", \"w\")\n",
    "    model_out_yaml.write(clf.model.to_yaml())\n",
    "    model_out_yaml.close()\n",
    "\n",
    "    print(\"Steps: Train: {0} Validation: {1}\".format(int(clf.params[\"samples_per_epoch\"]/clf.params[\"batch_size\"]), # CORRECTED\n",
    "                                               int(clf.params[\"samples_per_epoch_val\"]/clf.params[\"batch_size\"]))) # CORRECTED\n",
    "\n",
    "    ret = clf.model.fit_generator(train_gen,\n",
    "                                  steps_per_epoch = clf.params[\"samples_per_epoch\"]/clf.params[\"batch_size\"],\n",
    "                                  validation_steps = clf.params[\"samples_per_epoch_val\"]/clf.params[\"batch_size\"], # CORRECTED\n",
    "                                  #verbose=0, \n",
    "                                  epochs = clf.params[\"nb_epoch\"],\n",
    "                                  validation_data=val_gen, # CORRECTED\n",
    "                                  callbacks = [checkpoint, early_stop, LossPlotter(outdir)])\n",
    "    \n",
    "    print(\"fit Done\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(ret.history[\"acc\"])\n",
    "    plt.plot(ret.history[\"val_acc\"])\n",
    "    plt.savefig(outdir + \"/acc.png\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(ret.history[\"loss\"])\n",
    "    plt.plot(ret.history[\"val_loss\"])\n",
    "    plt.savefig(outdir + \"/loss.png\")\n",
    "\n",
    "    #valacc_out = open(outdir + \"/valacc.txt\", \"w\")\n",
    "    #valacc_out.write(str(ret.history[\"val_acc\"][-1]) + \"\\n\")\n",
    "    #valacc_out.close()\n",
    "\n",
    "    #maxvalacc_out = open(outdir + \"/maxvalacc.txt\", \"w\")\n",
    "    #maxvalacc_out.write(str(max(ret.history[\"val_acc\"])) + \"\\n\")\n",
    "    #maxvalacc_out.close()\n",
    "  \n",
    "    #deltaacc_out = open(outdir + \"/\" + clf.name + \"deltaacc.txt\", \"w\")\n",
    "    #deltaacc_out.write(str(ret.history[\"val_acc\"][-1] - ret.history[\"acc\"][-1]) + \"\\n\")\n",
    "    #deltaacc_out.close()\n",
    "\n",
    "    # save the architecture\n",
    "    model_out_yaml = open(outdir + \"/\" + clf.name + \".yaml\", \"w\")\n",
    "    model_out_yaml.write(clf.model.to_yaml())\n",
    "    model_out_yaml.close()\n",
    "  \n",
    "    # And the weights\n",
    "    clf.model.save_weights(outdir + \"/\" + clf.name + '_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0354ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(self):\n",
    "    if not self.load_from_file:\n",
    "        if self.backend == \"scikit\":\n",
    "            train_scikit(self)\n",
    "        elif self.backend == \"keras\":\n",
    "            return train_keras(self)\n",
    "    else:\n",
    "        if self.backend == \"scikit\":\n",
    "            f = open(os.path.join(self.inpath,self.name + \".pickle\"), \"r\")\n",
    "            self.model = pickle.load(f)\n",
    "            f.close()\n",
    "        elif self.backend == \"keras\":\n",
    "            print(\"Loading\", self.name)\n",
    "            f = open(os.path.join(self.inpath,self.name + \".yaml\"), \"r\")\n",
    "            yaml_string = f.read()\n",
    "            f.close()       \n",
    "            print(\"Getting yaml\")\n",
    "            self.model = model_from_yaml(yaml_string,  custom_objects={\"LoLa\":LoLa, \"Convert\":Convert})                \n",
    "            print(\"Got yaml\")\n",
    "            self.model.load_weights(os.path.join(self.inpath,self.name + \"_weights.h5\"))       \n",
    "            print(\"Loading\", self.name, \"from file: Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch in range(nbatches):\n",
    "    df = next(clf.datagen_test) # CORRECTED\n",
    "    if clf.backend == \"keras\":\n",
    "        X = clf.image_fun(df)        \n",
    "        probs = clf.model.predict_on_batch(X)\n",
    "    else:        \n",
    "        X = get_data_vars(df, clf.varlist)\n",
    "        probs = clf.model.predict_proba(X)\n",
    " \n",
    "        probs = np.nan_to_num(probs)\n",
    "\n",
    "        # prediction returns two values: \n",
    "        # signal and background probability\n",
    "        # we're just interested in the signal prob (bg prob = 1 - signal_prob)        \n",
    "        df[\"sigprob_\" + clf.name] = probs[:,1] \n",
    "\n",
    "        # Label is maximum classiier\n",
    "        df[\"label_\" + clf.name]   = np.apply_along_axis(lambda x:int(np.argmax(x)),1,probs)\n",
    "                \n",
    "        # Add per-class probs\n",
    "        probnames= []\n",
    "        for iclass in range(clf.params[\"n_classes\"]):\n",
    "            probname = \"cprob_{0}_{1}\".format(iclass, clf.name)            \n",
    "            df[probname] = probs[:,iclass]\n",
    "            probnames.append(probname)\n",
    "            \n",
    "        # Now that we have calculated the classifier response, \n",
    "        # remove the rest\n",
    "        cols_to_keep = set([\"entry\", \n",
    "                            clf.params[\"signal_branch\"], \n",
    "                            \"label_\" + clf.name,\n",
    "                            \"sigprob_\" + clf.name] + probnames )\n",
    "        cols_to_drop = list(set(df.columns) - cols_to_keep)\n",
    "        df = df.drop(cols_to_drop,axis=1)\n",
    "\n",
    "        df_all = df_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81490335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4e4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7d9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6601907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97d201",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatches = int(clf.params[\"samples_per_epoch_test\"]/clf.params[\"batch_size\"] - 1) # CORRECTED\n",
    "\n",
    "    # Enable to print weights\n",
    "    if False: \n",
    "        for layer in clf.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            print(weights)\n",
    "\n",
    "\n",
    "    df_all = pandas.DataFrame()\n",
    "    \n",
    "    # Loop over batches\n",
    "    for i_batch in range(nbatches):\n",
    "\n",
    "        df = next(clf.datagen_test) # CORRECTED\n",
    "\n",
    "        if clf.backend == \"keras\":\n",
    "            X = clf.image_fun(df)        \n",
    "            probs = clf.model.predict_on_batch(X)\n",
    "        else:        \n",
    "            X = get_data_vars(df, clf.varlist)\n",
    "            probs = clf.model.predict_proba(X)\n",
    " \n",
    "        probs = np.nan_to_num(probs)\n",
    "\n",
    "        # prediction returns two values: \n",
    "        # signal and background probability\n",
    "        # we're just interested in the signal prob (bg prob = 1 - signal_prob)        \n",
    "        df[\"sigprob_\" + clf.name] = probs[:,1] \n",
    "\n",
    "        # Label is maximum classiier\n",
    "        df[\"label_\" + clf.name]   = np.apply_along_axis(lambda x:int(np.argmax(x)),1,probs)\n",
    "                \n",
    "        # Add per-class probs\n",
    "        probnames= []\n",
    "        for iclass in range(clf.params[\"n_classes\"]):\n",
    "            probname = \"cprob_{0}_{1}\".format(iclass, clf.name)            \n",
    "            df[probname] = probs[:,iclass]\n",
    "            probnames.append(probname)\n",
    "            \n",
    "        # Now that we have calculated the classifier response, \n",
    "        # remove the rest\n",
    "        cols_to_keep = set([\"entry\", \n",
    "                            clf.params[\"signal_branch\"], \n",
    "                            \"label_\" + clf.name,\n",
    "                            \"sigprob_\" + clf.name] + probnames )\n",
    "        cols_to_drop = list(set(df.columns) - cols_to_keep)\n",
    "        df = df.drop(cols_to_drop,axis=1)\n",
    "\n",
    "        df_all = df_all.append(df)\n",
    "\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    #ll = log_loss(df_all[clf.params[\"signal_branch\"]], df_all[probnames])\n",
    "    #print(\"Log loss: {0}\".format(ll))\n",
    "\n",
    "    cm = confusion_matrix(df_all[clf.params[\"signal_branch\"]], df_all[\"label_\" + clf.name])\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Area under curve\n",
    "    AUC = roc_auc_score(df_all[clf.params[\"signal_branch\"]], df_all[\"sigprob_\" + clf.name])\n",
    "    print(\"AUC: {0}\".format(AUC))\n",
    "\n",
    "    # Mutual Information\n",
    "    mi = max([mutual_info_score(df_all[\"is_signal_new\"], df_all[\"sigprob_\" + clf.name]<thres) for thres in np.arange(0,1,0.01)])\n",
    "    print(\"MI: {0}\".format(mi))\n",
    "\n",
    "    # Normalized Mutual Information\n",
    "    nmi = max([normalized_mutual_info_score(df_all[\"is_signal_new\"], df_all[\"sigprob_\" + clf.name]<thres) for thres in np.arange(0,1,0.01)])\n",
    "    print(\"NMI: {0}\".format(nmi))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(df_all[clf.params[\"signal_branch\"]], df_all[\"sigprob_\" + clf.name])\n",
    "\n",
    "    outdir = clf.params[\"output_path\"] + clf.name\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {0:.2f})'.format(AUC))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(outdir + \"/roc.png\")\n",
    "\n",
    "\n",
    "    store_df = pandas.HDFStore('output_' + clf.name + suffix + '.h5')\n",
    "    store_df[\"all\"] = df_all\n",
    "    store_df.close()\n",
    "    \n",
    "    return -1. * AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        TCB.Classifier(params[\"model_name\"],\n",
    "                   \"keras\",\n",
    "                   params,\n",
    "                   params[\"read_from_file\"],\n",
    "                   datagen_train_pixel,                   \n",
    "                   datagen_test_pixel,               \n",
    "                   datagen_val_pixel,               \n",
    "                   the_model(params),\n",
    "                   image_fun = the_image_fun,           \n",
    "                   class_names = class_names,\n",
    "                   inpath = \"Lola_Poly2/\",\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb0bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd73dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"inputs\"] == \"2d\" or params[\"inputs\"] == \"caps\":\n",
    "    pixel_brs += [\"img_{0}\".format(i) for i in range(40*40)]\n",
    "elif params[\"inputs\"] == \"constit_fcn\":\n",
    "    pixel_brs += [\"{0}_{1}\".format(feature,constit) for feature in feat_list for constit in range(params[\"n_constit\"])]\n",
    "elif params[\"inputs\"] == \"constit_lola\":\n",
    "    pixel_brs += [\"{0}_{1}\".format(feature,constit) for feature in feat_list for constit in range(params[\"n_constit\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a35a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabe5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ac025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac2f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045cf925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
