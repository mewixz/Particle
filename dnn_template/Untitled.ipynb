{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b23aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefd04ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Data Preparing\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Data Preparing\")\n",
    "pixels = [\"img_{0}\".format(i) for i in range(1600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81754ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_image(df):\n",
    "    return np.expand_dims(df[pixels], axis=-1).reshape(-1, 40,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3643fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_image_2d(df):\n",
    "    foo =  np.expand_dims(np.expand_dims(df[ [\"img_{0}\".format(i) for i in range(40*40)]], axis=-1).reshape(-1,40,40), axis=-1)        \n",
    "    return foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b58511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f050371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_train = pd.HDFStore('../train_image.h5')\n",
    "df_train_images = store_train.select(\"table\",stop=1200000)\n",
    "n_train_samples = int((store_train.get_storer('table').nrows/batch_size))*batch_size\n",
    "\n",
    "\n",
    "store_val = pd.HDFStore('../val_image.h5')\n",
    "df_val_images = store_val.select(\"table\",stop=1200000)\n",
    "n_val_samples = int((store_val.get_storer('table').nrows/batch_size))*batch_size\n",
    "\n",
    "store_test = pd.HDFStore('../test_image.h5')\n",
    "df_test_images = store_test.select(\"table\",stop=1200000)\n",
    "n_test_samples = int((store_test.get_storer('table').nrows/batch_size))*batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb3cfc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_0</th>\n",
       "      <th>img_1</th>\n",
       "      <th>img_2</th>\n",
       "      <th>img_3</th>\n",
       "      <th>img_4</th>\n",
       "      <th>img_5</th>\n",
       "      <th>img_6</th>\n",
       "      <th>img_7</th>\n",
       "      <th>img_8</th>\n",
       "      <th>img_9</th>\n",
       "      <th>...</th>\n",
       "      <th>img_1592</th>\n",
       "      <th>img_1593</th>\n",
       "      <th>img_1594</th>\n",
       "      <th>img_1595</th>\n",
       "      <th>img_1596</th>\n",
       "      <th>img_1597</th>\n",
       "      <th>img_1598</th>\n",
       "      <th>img_1599</th>\n",
       "      <th>is_signal_new</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.394016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.997341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.648197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.098465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>174.350220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   img_0  img_1  img_2  img_3  img_4  img_5  img_6  img_7  img_8  img_9  ...  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  ...   \n",
       "\n",
       "   img_1592  img_1593  img_1594  img_1595  img_1596  img_1597  img_1598  \\\n",
       "0       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "1       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   img_1599  is_signal_new        mass  \n",
       "0       0.0            0.0   37.394016  \n",
       "1       0.0            0.0   46.997341  \n",
       "2       0.0            0.0   61.648197  \n",
       "3       0.0            0.0   52.098465  \n",
       "4       0.0            0.0  174.350220  \n",
       "\n",
       "[5 rows x 1602 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94b407f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_0</th>\n",
       "      <th>img_1</th>\n",
       "      <th>img_2</th>\n",
       "      <th>img_3</th>\n",
       "      <th>img_4</th>\n",
       "      <th>img_5</th>\n",
       "      <th>img_6</th>\n",
       "      <th>img_7</th>\n",
       "      <th>img_8</th>\n",
       "      <th>img_9</th>\n",
       "      <th>...</th>\n",
       "      <th>img_1592</th>\n",
       "      <th>img_1593</th>\n",
       "      <th>img_1594</th>\n",
       "      <th>img_1595</th>\n",
       "      <th>img_1596</th>\n",
       "      <th>img_1597</th>\n",
       "      <th>img_1598</th>\n",
       "      <th>img_1599</th>\n",
       "      <th>is_signal_new</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43221</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.839817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.690308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78615</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.865814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26383</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.494675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>111.776985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       img_0  img_1  img_2  img_3  img_4  img_5  img_6  img_7  img_8  img_9  \\\n",
       "43221    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "77280    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "78615    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "26383    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "789      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "       ...  img_1592  img_1593  img_1594  img_1595  img_1596  img_1597  \\\n",
       "43221  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "77280  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "78615  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "26383  ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "789    ...       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       img_1598  img_1599  is_signal_new        mass  \n",
       "43221       0.0       0.0            0.0   42.839817  \n",
       "77280       0.0       0.0            0.0   71.690308  \n",
       "78615       0.0       0.0            0.0  192.865814  \n",
       "26383       0.0       0.0            0.0  111.494675  \n",
       "789         0.0       0.0            0.0  111.776985  \n",
       "\n",
       "[5 rows x 1602 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f754fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-20 00:18:01.568437: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-20 00:18:01.568854: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    }
   ],
   "source": [
    "validation_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_val_images[pixels].values.reshape(-1, 40, 40), tf.float32),\n",
    "            tf.cast(df_val_images['is_signal_new'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5911da15",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\n\u001b[1;32m      3\u001b[0m         (\n\u001b[0;32m----> 4\u001b[0m             \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpixels\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m             tf\u001b[38;5;241m.\u001b[39mcast(df_train_images[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_signal_new\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      6\u001b[0m         )\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_env/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "training_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(df_train_images[pixels].values.reshape(-1, 40, 40), tf.float32),\n",
    "            tf.cast(df_train_images['is_signal_new'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e475ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bd51c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2d(params):\n",
    "    activ = lambda : Activation('relu')\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\",input_shape=(40, 40, 1)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "    model.add(layers.Conv2D(32,(3,3),padding='same',data_format='channels_last', activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(400,activation=\"relu\"))\n",
    "    model.add(layers.Dense(200,activation=\"relu\"))\n",
    "    model.add(layers.Dense(100,activation=\"relu\"))\n",
    "    model.add(layers.Dense(50,activation=\"relu\"))\n",
    "    model.add(layers.Dense(10,activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dense(nclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3678205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {}\n",
    "for i in range(2):\n",
    "    class_names[i] = \"c{0}\".format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cdc38b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'c0', 1: 'c1'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dff72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf0a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "                \n",
    "train_gen = generator(clf.datagen_train)\n",
    "val_gen  = generator(clf.datagen_val) # CORRECTED\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                               patience=clf.params[\"early_stop_patience\"], \n",
    "                               verbose=0, \n",
    "                               mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keras(clf):\n",
    "\n",
    "    print(\"Starting train_keras with the parameters: \")\n",
    "    for k,v in clf.params.items():\n",
    "        print(\"\\t\", k,\"=\",v)\n",
    "\n",
    "    outdir = clf.params[\"output_path\"] + clf.name\n",
    "\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "      \n",
    "    # Prepare model and train\n",
    "    #sgd = SGD(lr = clf.params[\"lr\"], \n",
    "    #          decay = clf.params[\"decay\"], \n",
    "    #          momentum = clf.params[\"momentum\"], \n",
    "    #          nesterov=True)\n",
    "    clf.model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=\"adam\", \n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    if clf.params[\"reweight_events\"]:\n",
    "        f = open(\"mass_weights.pickle\", \"rb\")\n",
    "        di = pickle.load(f)\n",
    "        \n",
    "        counts_qcd  = di[\"counts_qcd\"]\n",
    "        bins_qcd    = di[\"bins_qcd\"]\n",
    "        counts_top  = di[\"counts_top\"]\n",
    "        bins_top    = di[\"bins_top\"]\n",
    "       \n",
    "        \n",
    "        bins_qcd = bins_qcd[:-1]+(bins_qcd[1:]-bins_qcd[:-1])/2.\n",
    "        bins_top = bins_top[:-1]+(bins_top[1:]-bins_top[:-1])/2.\n",
    "     \n",
    "        bins = [bins_qcd, bins_top]\n",
    "        counts = [counts_qcd, counts_top]\n",
    "        \n",
    "        c_min =[min(counts[0][np.nonzero(counts[0])]), min(counts[1][np.nonzero(counts[1])])]\n",
    "\n",
    "        def calc_w(m,s):\n",
    "\n",
    "            if m>max(bins[s]) or m<min(bins[s]):\n",
    "                return 1/c_min[s]\n",
    "\n",
    "            c = counts[s][np.abs(m-bins[s]).argmin()]\n",
    "            c = max(c_min[s],c)\n",
    "\n",
    "            return 1/c\n",
    "\n",
    "\n",
    "    print(\"Calling fit_generator\")\n",
    "\n",
    "    def generator(dg):\n",
    "        while True:\n",
    "            df = next(dg)\n",
    "            \n",
    "            # Shuffle\n",
    "            df = df.iloc[np.random.permutation(len(df))]\n",
    "\n",
    "            X = clf.image_fun(df)\n",
    "            y = np_utils.to_categorical(df[clf.params[\"signal_branch\"]].values,clf.params[\"n_classes\"])\n",
    "\n",
    "            if clf.params[\"reweight_events\"]:\n",
    "                \n",
    "                Es = [\"E_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PXs = [\"PX_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PYs = [\"PY_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "                PZs = [\"PZ_{0}\".format(i) for i in range(clf.params[\"n_constit\"])]\n",
    "\n",
    "                E_tot = df[Es].sum(axis=1)\n",
    "                PX_tot = df[PXs].sum(axis=1)\n",
    "                PY_tot = df[PYs].sum(axis=1)\n",
    "                PZ_tot = df[PZs].sum(axis=1)\n",
    "\n",
    "                M = np.sqrt(np.power(E_tot,2) - np.power(PX_tot,2) - np.power(PY_tot,2) - np.power(PZ_tot,2))\n",
    "                w = np.array([calc_w(m,s) for m,s in zip(M,df[\"is_signal_new\"])])\n",
    "                        \n",
    "                yield  X,y, w\n",
    "\n",
    "            else:        \n",
    "                yield X,y\n",
    "\n",
    "                \n",
    "    train_gen = generator(clf.datagen_train)\n",
    "    val_gen  = generator(clf.datagen_val) # CORRECTED\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', \n",
    "                               patience=clf.params[\"early_stop_patience\"], \n",
    "                               verbose=0, \n",
    "                               mode='auto')\n",
    "\n",
    "    filepath= outdir + \"/weights-latest.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "    # save the architecture\n",
    "    model_out_yaml = open(outdir + \"/\" + clf.name + \".yaml\", \"w\")\n",
    "    model_out_yaml.write(clf.model.to_yaml())\n",
    "    model_out_yaml.close()\n",
    "\n",
    "    print(\"Steps: Train: {0} Validation: {1}\".format(int(clf.params[\"samples_per_epoch\"]/clf.params[\"batch_size\"]), # CORRECTED\n",
    "                                               int(clf.params[\"samples_per_epoch_val\"]/clf.params[\"batch_size\"]))) # CORRECTED\n",
    "\n",
    "    ret = clf.model.fit_generator(train_gen,\n",
    "                                  steps_per_epoch = clf.params[\"samples_per_epoch\"]/clf.params[\"batch_size\"],\n",
    "                                  validation_steps = clf.params[\"samples_per_epoch_val\"]/clf.params[\"batch_size\"], # CORRECTED\n",
    "                                  #verbose=0, \n",
    "                                  epochs = clf.params[\"nb_epoch\"],\n",
    "                                  validation_data=val_gen, # CORRECTED\n",
    "                                  callbacks = [checkpoint, early_stop, LossPlotter(outdir)])\n",
    "    \n",
    "    print(\"fit Done\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(ret.history[\"acc\"])\n",
    "    plt.plot(ret.history[\"val_acc\"])\n",
    "    plt.savefig(outdir + \"/acc.png\")\n",
    "\n",
    "    plt.clf()\n",
    "    plt.plot(ret.history[\"loss\"])\n",
    "    plt.plot(ret.history[\"val_loss\"])\n",
    "    plt.savefig(outdir + \"/loss.png\")\n",
    "\n",
    "    #valacc_out = open(outdir + \"/valacc.txt\", \"w\")\n",
    "    #valacc_out.write(str(ret.history[\"val_acc\"][-1]) + \"\\n\")\n",
    "    #valacc_out.close()\n",
    "\n",
    "    #maxvalacc_out = open(outdir + \"/maxvalacc.txt\", \"w\")\n",
    "    #maxvalacc_out.write(str(max(ret.history[\"val_acc\"])) + \"\\n\")\n",
    "    #maxvalacc_out.close()\n",
    "  \n",
    "    #deltaacc_out = open(outdir + \"/\" + clf.name + \"deltaacc.txt\", \"w\")\n",
    "    #deltaacc_out.write(str(ret.history[\"val_acc\"][-1] - ret.history[\"acc\"][-1]) + \"\\n\")\n",
    "    #deltaacc_out.close()\n",
    "\n",
    "    # save the architecture\n",
    "    model_out_yaml = open(outdir + \"/\" + clf.name + \".yaml\", \"w\")\n",
    "    model_out_yaml.write(clf.model.to_yaml())\n",
    "    model_out_yaml.close()\n",
    "  \n",
    "    # And the weights\n",
    "    clf.model.save_weights(outdir + \"/\" + clf.name + '_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3932b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(self):\n",
    "    if not self.load_from_file:\n",
    "        if self.backend == \"scikit\":\n",
    "            train_scikit(self)\n",
    "        elif self.backend == \"keras\":\n",
    "            return train_keras(self)\n",
    "    else:\n",
    "        if self.backend == \"scikit\":\n",
    "            f = open(os.path.join(self.inpath,self.name + \".pickle\"), \"r\")\n",
    "            self.model = pickle.load(f)\n",
    "            f.close()\n",
    "        elif self.backend == \"keras\":\n",
    "            print(\"Loading\", self.name)\n",
    "            f = open(os.path.join(self.inpath,self.name + \".yaml\"), \"r\")\n",
    "            yaml_string = f.read()\n",
    "            f.close()       \n",
    "            print(\"Getting yaml\")\n",
    "            self.model = model_from_yaml(yaml_string,  custom_objects={\"LoLa\":LoLa, \"Convert\":Convert})                \n",
    "            print(\"Got yaml\")\n",
    "            self.model.load_weights(os.path.join(self.inpath,self.name + \"_weights.h5\"))       \n",
    "            print(\"Loading\", self.name, \"from file: Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch in range(nbatches):\n",
    "    df = next(clf.datagen_test) # CORRECTED\n",
    "    if clf.backend == \"keras\":\n",
    "        X = clf.image_fun(df)        \n",
    "        probs = clf.model.predict_on_batch(X)\n",
    "    else:        \n",
    "        X = get_data_vars(df, clf.varlist)\n",
    "        probs = clf.model.predict_proba(X)\n",
    " \n",
    "        probs = np.nan_to_num(probs)\n",
    "\n",
    "        # prediction returns two values: \n",
    "        # signal and background probability\n",
    "        # we're just interested in the signal prob (bg prob = 1 - signal_prob)        \n",
    "        df[\"sigprob_\" + clf.name] = probs[:,1] \n",
    "\n",
    "        # Label is maximum classiier\n",
    "        df[\"label_\" + clf.name]   = np.apply_along_axis(lambda x:int(np.argmax(x)),1,probs)\n",
    "                \n",
    "        # Add per-class probs\n",
    "        probnames= []\n",
    "        for iclass in range(clf.params[\"n_classes\"]):\n",
    "            probname = \"cprob_{0}_{1}\".format(iclass, clf.name)            \n",
    "            df[probname] = probs[:,iclass]\n",
    "            probnames.append(probname)\n",
    "            \n",
    "        # Now that we have calculated the classifier response, \n",
    "        # remove the rest\n",
    "        cols_to_keep = set([\"entry\", \n",
    "                            clf.params[\"signal_branch\"], \n",
    "                            \"label_\" + clf.name,\n",
    "                            \"sigprob_\" + clf.name] + probnames )\n",
    "        cols_to_drop = list(set(df.columns) - cols_to_keep)\n",
    "        df = df.drop(cols_to_drop,axis=1)\n",
    "\n",
    "        df_all = df_all.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed290f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff84a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60752fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752c152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a677ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbatches = int(clf.params[\"samples_per_epoch_test\"]/clf.params[\"batch_size\"] - 1) # CORRECTED\n",
    "\n",
    "    # Enable to print weights\n",
    "    if False: \n",
    "        for layer in clf.model.layers:\n",
    "            weights = layer.get_weights()\n",
    "            print(weights)\n",
    "\n",
    "\n",
    "    df_all = pandas.DataFrame()\n",
    "    \n",
    "    # Loop over batches\n",
    "    for i_batch in range(nbatches):\n",
    "\n",
    "        df = next(clf.datagen_test) # CORRECTED\n",
    "\n",
    "        if clf.backend == \"keras\":\n",
    "            X = clf.image_fun(df)        \n",
    "            probs = clf.model.predict_on_batch(X)\n",
    "        else:        \n",
    "            X = get_data_vars(df, clf.varlist)\n",
    "            probs = clf.model.predict_proba(X)\n",
    " \n",
    "        probs = np.nan_to_num(probs)\n",
    "\n",
    "        # prediction returns two values: \n",
    "        # signal and background probability\n",
    "        # we're just interested in the signal prob (bg prob = 1 - signal_prob)        \n",
    "        df[\"sigprob_\" + clf.name] = probs[:,1] \n",
    "\n",
    "        # Label is maximum classiier\n",
    "        df[\"label_\" + clf.name]   = np.apply_along_axis(lambda x:int(np.argmax(x)),1,probs)\n",
    "                \n",
    "        # Add per-class probs\n",
    "        probnames= []\n",
    "        for iclass in range(clf.params[\"n_classes\"]):\n",
    "            probname = \"cprob_{0}_{1}\".format(iclass, clf.name)            \n",
    "            df[probname] = probs[:,iclass]\n",
    "            probnames.append(probname)\n",
    "            \n",
    "        # Now that we have calculated the classifier response, \n",
    "        # remove the rest\n",
    "        cols_to_keep = set([\"entry\", \n",
    "                            clf.params[\"signal_branch\"], \n",
    "                            \"label_\" + clf.name,\n",
    "                            \"sigprob_\" + clf.name] + probnames )\n",
    "        cols_to_drop = list(set(df.columns) - cols_to_keep)\n",
    "        df = df.drop(cols_to_drop,axis=1)\n",
    "\n",
    "        df_all = df_all.append(df)\n",
    "\n",
    "    #pdb.set_trace()\n",
    "\n",
    "    #ll = log_loss(df_all[clf.params[\"signal_branch\"]], df_all[probnames])\n",
    "    #print(\"Log loss: {0}\".format(ll))\n",
    "\n",
    "    cm = confusion_matrix(df_all[clf.params[\"signal_branch\"]], df_all[\"label_\" + clf.name])\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Area under curve\n",
    "    AUC = roc_auc_score(df_all[clf.params[\"signal_branch\"]], df_all[\"sigprob_\" + clf.name])\n",
    "    print(\"AUC: {0}\".format(AUC))\n",
    "\n",
    "    # Mutual Information\n",
    "    mi = max([mutual_info_score(df_all[\"is_signal_new\"], df_all[\"sigprob_\" + clf.name]<thres) for thres in np.arange(0,1,0.01)])\n",
    "    print(\"MI: {0}\".format(mi))\n",
    "\n",
    "    # Normalized Mutual Information\n",
    "    nmi = max([normalized_mutual_info_score(df_all[\"is_signal_new\"], df_all[\"sigprob_\" + clf.name]<thres) for thres in np.arange(0,1,0.01)])\n",
    "    print(\"NMI: {0}\".format(nmi))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(df_all[clf.params[\"signal_branch\"]], df_all[\"sigprob_\" + clf.name])\n",
    "\n",
    "    outdir = clf.params[\"output_path\"] + clf.name\n",
    "    plt.clf()\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {0:.2f})'.format(AUC))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(outdir + \"/roc.png\")\n",
    "\n",
    "\n",
    "    store_df = pandas.HDFStore('output_' + clf.name + suffix + '.h5')\n",
    "    store_df[\"all\"] = df_all\n",
    "    store_df.close()\n",
    "    \n",
    "    return -1. * AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd8a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "        TCB.Classifier(params[\"model_name\"],\n",
    "                   \"keras\",\n",
    "                   params,\n",
    "                   params[\"read_from_file\"],\n",
    "                   datagen_train_pixel,                   \n",
    "                   datagen_test_pixel,               \n",
    "                   datagen_val_pixel,               \n",
    "                   the_model(params),\n",
    "                   image_fun = the_image_fun,           \n",
    "                   class_names = class_names,\n",
    "                   inpath = \"Lola_Poly2/\",\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d902d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"inputs\"] == \"2d\" or params[\"inputs\"] == \"caps\":\n",
    "    pixel_brs += [\"img_{0}\".format(i) for i in range(40*40)]\n",
    "elif params[\"inputs\"] == \"constit_fcn\":\n",
    "    pixel_brs += [\"{0}_{1}\".format(feature,constit) for feature in feat_list for constit in range(params[\"n_constit\"])]\n",
    "elif params[\"inputs\"] == \"constit_lola\":\n",
    "    pixel_brs += [\"{0}_{1}\".format(feature,constit) for feature in feat_list for constit in range(params[\"n_constit\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d540ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5416a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76b1fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6090fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2754686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
